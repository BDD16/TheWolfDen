# JPEG and DNG Payload Extractor and Forensic Analyzer

This document describes the structure, usage, and forensic value of `payload_extractor.py`, a defensive analysis tool designed to extract embedded data and suspicious payloads from JPEG and Adobe DNG (TIFF+JPEG) images. It is designed to help security researchers detect malicious use of image metadata segments and trailing data, especially for vulnerabilities such as CVE-2025-43300.

## 1. Introduction

Images are a common carrier for embedded malicious payloads because formats like JPEG and DNG allow for metadata and segment based storage that is loosely validated by many decoders. Vulnerabilities such as CVE-2025-43300 exploit this leniency by embedding structured payloads within:

1. APPn (application-specific) metadata segments, such as APP0 through APP15
2. Trailing data after the official end-of-image (EOI) marker

This tool is designed for forensic analysis only. It does not attempt to exploit or execute any content. Purely to extract a potential payload, if it exists, but leaves the analyst or engineer
to determine if the payload is real and malicious.  Generally speaking, why would code be embedded
into a JPG or PNG if it was not malicious in some capacity?

## 2. Objectives

The script performs the following:

- Detects JPEG-like or TIFF-like structure
- Enumerates all JPEG markers, including their type, location, and size
- Extracts APPn payloads into separate files for manual analysis
- Identifies and extracts data appended after the EOI marker
- Produces a JSON summary report including heuristics such as large segments or suspicious trailing data

## 3. CVE-2025-43300 and Related Vulnerabilities

CVE-2025-43300 exploits a flaw in image decoder logic where oversized APPn segments or data following the EOI marker are incorrectly handled, leading to memory corruption or misinterpretation. This tool helps identify image files constructed to trigger such vulnerabilities by:

- Isolating suspicious segments
- Dumping appended binary blobs
- Avoiding parser based decoding which could trigger exploitation

This approach works because payloads must reside in defined offsets and follow a detectable structure to be processed by the decoder. This allows byte-level inspection and extraction to reveal hidden stages or embedded objects without triggering vulnerabilities.

## 4. Technical Overview

### 4.1 JPEG Marker Scanning

Each marker in a JPEG file begins with a 0xFF byte followed by a marker ID. The script walks through the binary data and identifies:

- Standalone markers (e.g., SOI 0xFFD8, EOI 0xFFD9)
- Markers with payloads and length fields (e.g., APP0-APP15, COM)
- Length values and actual payload bytes for segments that contain data

### 4.2 APPn Segment Extraction

Markers APP0 to APP15 (0xFFE0 to 0xFFEF) are commonly used for storing metadata like EXIF, JFIF, XMP, or ICC profiles. The script extracts these and:

- Saves each APP payload into a standalone binary file
- Calculates and stores SHA-256 hashes for identification
- Flags any segments exceeding a 50 KB size threshold for further inspection

### 4.3 TIFF and DNG Header Detection

DNG files typically begin with TIFF headers, which start with either 'II' (little endian) or 'MM' (big endian). The script checks for:

- TIFF magic number
- First IFD (Image File Directory) offset
- Byte ordering for TIFF parsing

### 4.4 Trailing Data Analysis

The end of a JPEG image is denoted by the EOI marker (0xFFD9). Malicious actors often append second-stage payloads, ZIP files, or encrypted content after this marker. The script checks for any data beyond EOI and extracts it as a separate file.

Heuristic flags are raised when:

- Trailing data exists beyond EOI
- Trailing content exceeds 16 bytes in length
- Oversized APPn segments are detected

### 4.5 Artifact Output

For each file analyzed, the script produces:

- A directory named after the input file
- A `summary.json` file detailing segment structures and heuristics
- One file per APPn payload (e.g., `app_chunk_0_0xe1.bin`)
- A `trailing_after_eoi.bin` file if such data exists

This structure allows you to inspect, scan, or feed each component to a separate analysis tool without having to trust a potentially vulnerable decoder.

## 5. Sample Use Case

```bash
python3 payload_extractor.py suspicious.jpg
```

A directory structure is then made as the following:

```bash
extracted_artifacts/
  suspicious.jpg/
    summary.json
    app_chunk_0_0xe1.bin
    app_chunk_1_0xee.bin
    trailing_after_eoi.bin
```
# 6. Defensive Use Cases

This tool is suitable for:

    - Malware triage during incident response
    - Extracting potentially harmful metadata from phishing attachments
    - Analyzing forensic disk images for embedded malware stages
    - Verifying image files during threat hunting
    - Detecting anomalies introduced by steganographic or obfuscation techniques

## 7. Artifact Stitching via Overlap and Entropy Heuristics

In some cases, malicious payloads embedded in APPn segments or after the EOI marker are deliberately fragmented to evade detection. These fragments may be separately encoded, compressed, or obfuscated, only becoming functional when reassembled.

The `stitch_artifacts.py` tool attempts to heuristically reconstruct full binary payloads by scanning extracted artifacts and greedily merging those with overlapping suffix-prefix byte sequences.

This technique is effective in uncovering payloads that are:

- Split across multiple JPEG APP segments
- Continued in trailing regions after the EOI
- Distributed in chunks to bypass size or entropy-based filters

### 7.1 Stitching Logic

The script performs the following operations:

1. **Reads all files** in a specified artifact directory (e.g., from `payload_extractor.py` output)
2. **Calculates entropy and sample headers** to provide analyst insight into each fragment
3. **Computes pairwise overlaps**:
    - For every pair of files `A` and `B`, the largest suffix of `A` is compared with the prefix of `B`
    - Overlaps up to a configurable `--max-overlap` value are checked
4. **Greedily merges** the pair with the largest exact overlap, only if it meets the minimum required overlap (`--min-overlap`)
5. **Repeats merging** until no more qualifying overlaps remain
6. **Writes output files** (`stitched_001.bin`, `stitched_002.bin`, etc.) to the destination directory
7. **Produces a manifest** listing each stitched output and the original fragments used to construct it

### 7.2 Command-line Usage

```bash
python stitch_artifacts.py output_artifacts/suspicious.jpg \
  --outdir output_artifacts/suspicious.jpg/stitched \
  --min-overlap 64 \
  --max-overlap 2048
```
# 9.3 Entropy and Header Inspection

The script prints a summary for each input file, including:

- Size in bytes

- Entropy score (0.0â€“8.0)

- First 16 bytes as hexadecimal

Fragments with low entropy and predictable headers (e.g., PK for ZIP, MZ for PE) often indicate staging payloads or embedded containers.

# 9.4 Why This Works for CVE-2025-43300

For CVE-2025-43300-style attacks, adversaries may:

- Spread payloads across multiple APP segments to avoid size detection

- Use short overlaps between fragments to defeat traditional scanning

- Append fragments after EOI that must be concatenated to reconstruct a ZIP archive or script

The stitching tool reconstructs likely payloads by mimicking decoder behavior that naively concatenates or interprets consecutive blocks. By recovering the full binary structure, analysts can apply file-type detection, YARA rules, or static analysis tools on the stitched results.


# 9.5 Cautions

This method uses exact byte matching only. It does not detect shifted, encoded, or compressed overlaps. Manual inspection is recommended for borderline cases.

Merges are not guaranteed to represent valid executable content but often provide a clearer signal for secondary tools such as binwalk, file, or disassemblers.

This stitching pipeline is valuable for forensic cases involving complex containerized payloads and should be included as a standard post-processing step in malicious image triage.

## 10. Conclusion

Malicious payloads embedded in multimedia formats represent a persistent and evolving threat vector,
especially as attackers exploit complex image standards like JPEG and DNG to bypass conventional
detection mechanisms. Vulnerabilities such as CVE-2025-43300 demonstrate how deeply embedded logic
flaws in image decoders can be weaponized to execute code or compromise system integrity using
seemingly innocuous files.

The tools and techniques described in this document provide a safe, decoder-independent method to
identify, extract, and analyze suspicious content hidden within image structures. By relying on
byte level inspection of JPEG markers, APPn segments, TIFF headers, and trailing data, defenders
can expose potential payloads without triggering vulnerable parsing logic.

In addition, the stitching utility enhances the analysis pipeline by enabling reconstruction of
fragmented or staged payloads that span multiple segments or regions. This is particularly useful
when an adversary deliberately splits their exploit chain across several embedded artifacts to
evade static signatures and size-based heuristics.

Together, the extraction and stitching phases form a robust, modular toolkit for malware analysts,
reverse engineers, and forensic responders tasked with examining untrusted images. These tools
emphasize transparency, reproducibility, and safety, ensuring that payloads are uncovered and
isolated for inspection without requiring any speculative execution or risky parsing behavior.

By combining static structure parsing with overlap-based reconstruction, analysts are equipped to
triage, correlate, and investigate image-based attacks in a scalable and automation-friendly
manner. As adversaries continue to explore the creative abuse of media formats, defenders must be 
equally innovative in exposing their methods and neutralizing their impact.


# appendix

## payload_extractor.py

```python
#!/usr/bin/env python3
"""
payload_extractor.py

Defensive and forensic helper for scanning JPEG / Adobe DNG (TIFF+JPEG) files for
anomalous or appended payloads. DOES NOT attempt to exploit anything.

Run in a sandbox/VM. Do NOT execute extracted payloads.

Outputs:
 - JSON summary printed to stdout
 - Extracted APPn chunks and trailing data saved under <out_dir>/<filename>/
"""

import os
import sys
import struct
import hashlib
import json

JPEG_MARKER_PREFIX = b'\xff'
SOI = b'\xff\xd8'
EOI = b'\xff\xd9'

def sha256_hex(b):
    return hashlib.sha256(b).hexdigest()

def read_u16_be(b, off):
    return struct.unpack_from('>H', b, off)[0]

def find_jpeg_markers(data):
    """
    Yield tuples (offset, marker_byte, length, payload_bytes)
    For markers that carry a 2-byte length (APPn, COM, etc), length includes the two length bytes.
    For SOS (start of scan), the payload continues until the next 0xff marker followed by 0xd9 (EOI),
    but here we yield the marker position; caller can handle compressed scan data detection.
    """
    i = 0
    n = len(data)
    while i < n - 1:
        if data[i] == 0xff:
            # skip padding 0xff bytes
            j = i + 1
            while j < n and data[j] == 0xff:
                j += 1
            if j >= n:
                break
            marker = data[j]
            marker_bytes = bytes([marker])
            marker_offset = i
            # standalone markers without length: SOI (D8), EOI (D9), RSTn (D0-D7)
            if marker in (0xd8, 0xd9) or (0xd0 <= marker <= 0xd7):
                yield (marker_offset, marker, 0, b'')
                i = j + 1
                continue
            # markers with length field
            if j + 2 <= n - 1:
                length = read_u16_be(data, j+1)
                payload_start = j+3
                payload_end = payload_start + (length - 2)
                if payload_end > n:
                    # truncated length: yield partial
                    payload = data[payload_start:n]
                else:
                    payload = data[payload_start:payload_end]
                yield (marker_offset, marker, length, payload)
                i = payload_end
                continue
            else:
                break
        else:
            i += 1

def extract_trailing_after_eoi(data):
    idx = data.find(EOI)
    if idx == -1:
        return None
    eoi_end = idx + 2
    if eoi_end >= len(data):
        return None
    trailing = data[eoi_end:]
    return eoi_end, trailing

def inspect_file(path, out_dir='extracted_artifacts'):
    basename = os.path.basename(path)
    with open(path, 'rb') as f:
        data = f.read()

    summary = {
        'file': basename,
        'size': len(data),
        'sha256': sha256_hex(data),
        'is_jpeg_like': False,
        'markers': [],
        'app_chunks': [],
        'trailing_after_eoi': False,
        'trailing_size': 0,
        'heuristics': []
    }

    # Quick check: is there a TIFF header (DNG often uses TIFF headers 'II' or 'MM')
    if data.startswith(b'II') or data.startswith(b'MM'):
        summary['tiff_like'] = True
        # Try to parse TIFF header: byte order + magic + first IFD offset
        try:
            byteorder = data[0:2]
            endian = '<' if byteorder == b'II' else '>'
            magic = struct.unpack_from(endian+'H', data, 2)[0]
            ifd0 = struct.unpack_from(endian+'I', data, 4)[0]
            summary['tiff_magic'] = int(magic)
            summary['tiff_ifd0_offset'] = int(ifd0)
            summary['heuristics'].append('tiff_header_present')
        except Exception as e:
            summary['tiff_parse_error'] = str(e)
    else:
        summary['tiff_like'] = False

    # Detect SOI
    if data.find(SOI) != -1:
        summary['is_jpeg_like'] = True

    # Walk JPEG markers
    markers = []
    for off, marker, length, payload in find_jpeg_markers(data):
        markers.append({
            'offset': off,
            'marker': hex(marker),
            'length_field': length,
            'payload_sha256': sha256_hex(payload) if payload else None,
            'payload_len': len(payload)
        })
        # record APPn
        if 0xe0 <= marker <= 0xef:  # APP0..APP15
            summary['app_chunks'].append({
                'offset': off,
                'marker': hex(marker),
                'length_field': length,
                'payload_len': len(payload),
                'payload_sha256': sha256_hex(payload) if payload else None
            })
    summary['markers'] = markers

    # Trailing data after EOI
    trailing = extract_trailing_after_eoi(data)
    if trailing:
        eoi_end, trailing_bytes = trailing
        summary['trailing_after_eoi'] = True
        summary['trailing_size'] = len(trailing_bytes)
        summary['heuristics'].append('trailing_data_after_eoi')
    else:
        summary['trailing_after_eoi'] = False

    # Heuristics: large APPn chunks, mismatches, anomalous trailing size
    for app in summary['app_chunks']:
        if app['payload_len'] > 1024 * 50:  # arbitrary threshold (50 KB)
            summary['heuristics'].append('large_app_chunk_{} at {}'.format(app['marker'], app['offset']))

    if summary['trailing_after_eoi'] and summary['trailing_size'] > 16:
        summary['heuristics'].append('nonempty_trailing_after_eoi')

    # Save extracted artifacts
    base_out = os.path.join(out_dir, basename)
    os.makedirs(base_out, exist_ok=True)
    # Save full file hash/summary
    summary_path = os.path.join(base_out, 'summary.json')
    with open(summary_path, 'w') as jf:
        json.dump(summary, jf, indent=2)

    # Save APP chunks as separate files
    for idx, app in enumerate(summary['app_chunks']):
        # find actual payload bytes again via offsets
        off = app['offset']
        # The payload starts after the 0xff marker and 2 byte length.
        # Re-parse to extract bytes reliably:
        # find marker position and marker byte
        if data[off:off+2][0] != 0xff:
            continue
        marker_byte = data[off+1]
        length_field = read_u16_be(data, off+2)
        payload_start = off + 4
        payload_end = payload_start + (length_field - 2)
        payload_bytes = data[payload_start:payload_end] if payload_end <= len(data) else data[payload_start:]
        out_name = os.path.join(base_out, f'app_chunk_{idx}_{hex(marker_byte)}.bin')
        with open(out_name, 'wb') as of:
            of.write(payload_bytes)

    # Save trailing bytes if present
    if summary['trailing_after_eoi']:
        eoi_pos = data.find(EOI)
        trailing_bytes = data[eoi_pos+2:]
        out_trail = os.path.join(base_out, 'trailing_after_eoi.bin')
        with open(out_trail, 'wb') as tf:
            tf.write(trailing_bytes)

    return summary, base_out

if __name__ == '__main__':
    if len(sys.argv) < 2:
        print("Usage: {} <image-file> [out_dir]".format(sys.argv[0]))
        sys.exit(2)
    path = sys.argv[1]
    od = sys.argv[2] if len(sys.argv) >= 3 else 'extracted_artifacts'
    s, out = inspect_file(path, od)
    print(json.dumps(s, indent=2))
    print("Extracted artifacts and summary written to:", out)

```

## stitch_artifacts.py

```python
#!/usr/bin/env python3
"""
stitch_artifacts.py

Usage:
    python stitch_artifacts.py /path/to/output_artifacts [--outdir output_artifacts/stitched] [--min-overlap 32] [--max-overlap 4096]

What it does:
 - Scans all files directly in the given artifacts directory.
 - Computes simple metadata (size, sample header, entropy).
 - Computes pairwise suffix-prefix exact overlaps (up to max_overlap bytes).
 - Greedily merges the pair with the largest overlap >= min_overlap, repeating until no pair meets threshold.
 - Writes merged files into outdir and a manifest CSV describing merges.

Note: This is a heuristic assembler. Always manually inspect stitched outputs.
"""
from __future__ import annotations
import argparse, os, math, csv
from pathlib import Path
from collections import Counter, namedtuple

FileMeta = namedtuple('FileMeta', ['path','size','header_hex','entropy'])

def entropy(data: bytes) -> float:
    if not data:
        return 0.0
    c = Counter(data)
    L = len(data)
    return -sum((v/L) * math.log2(v/L) for v in c.values())

def sample_header_hex(data: bytes, n=16) -> str:
    return data[:n].hex()

def read_files_from_dir(d: Path):
    files = []
    for p in sorted(d.iterdir()):
        if p.is_file():
            b = p.read_bytes()
            files.append((p, b))
    return files

def max_suffix_prefix_overlap(a: bytes, b: bytes, max_overlap:int):
    """
    Return the length of the largest suffix of a that is equal to a prefix of b (<= max_overlap).
    Implemented with a simple loop (fast enough for moderate sizes).
    """
    max_ol = 0
    # limit possible overlap by lengths
    limit = min(len(a), len(b), max_overlap)
    # search from large to small for early exit on large overlap (important)
    for l in range(limit, 0, -1):
        if a[-l:] == b[:l]:
            return l
    return 0

def greedy_assemble(files_bytes, min_overlap=32, max_overlap=4096):
    """
    files_bytes: list of (Path or id, bytes)
    returns list of assembled (name, bytes, sources)
    """
    # store as list of dicts for mutability
    fragments = [{'id': i, 'name': str(path), 'bytes': b, 'sources': [str(path)]} for i,(path,b) in enumerate(files_bytes)]
    next_id = len(fragments)
    while True:
        best = None  # (ol_len, i, j)
        n = len(fragments)
        if n < 2:
            break
        # compute pairwise overlaps
        for i in range(n):
            ai = fragments[i]['bytes']
            if len(ai) < min_overlap:
                continue
            for j in range(n):
                if i == j: 
                    continue
                bj = fragments[j]['bytes']
                if len(bj) < min_overlap:
                    continue
                ol = max_suffix_prefix_overlap(ai, bj, max_overlap)
                if ol >= min_overlap:
                    if best is None or ol > best[0]:
                        best = (ol, i, j)
        if not best:
            break
        ol, i, j = best
        A = fragments[i]
        B = fragments[j]
        merged_bytes = A['bytes'] + B['bytes'][ol:]
        merged_sources = A['sources'] + B['sources']
        merged_name = f"merged_{next_id}"
        print(f"Merging {A['name']} + {B['name']} with overlap {ol} -> {merged_name}")
        # remove i and j (ensure remove larger index first)
        for idx in sorted([i,j], reverse=True):
            fragments.pop(idx)
        fragments.append({'id': next_id, 'name': merged_name, 'bytes': merged_bytes, 'sources': merged_sources})
        next_id += 1
    return fragments

def main():
    ap = argparse.ArgumentParser(description="Attempt to stitch artifact fragments by suffix-prefix overlap.")
    ap.add_argument("artifacts_dir", help="Directory containing artifact files (single-level).")
    ap.add_argument("--outdir", "-o", help="Directory to save stitched outputs (default: output_artifacts/stitched)", default="output_artifacts/stitched")
    ap.add_argument("--min-overlap", type=int, default=32, help="Minimum overlap in bytes to consider a valid merge (default 32)")
    ap.add_argument("--max-overlap", type=int, default=4096, help="Max overlap to search for (default 4096)")
    args = ap.parse_args()

    artifacts_dir = Path(args.artifacts_dir)
    if not artifacts_dir.exists():
        print("Error: artifacts dir not found:", artifacts_dir); return
    files = read_files_from_dir(artifacts_dir)
    if not files:
        print("No files found in", artifacts_dir); return

    # metadata
    metas = []
    files_bytes = []
    for p,b in files:
        metas.append(FileMeta(path=str(p), size=len(b), header_hex=sample_header_hex(b,16), entropy=entropy(b)))
        files_bytes.append((p,b))

    # print quick summary
    print("Found files:", len(files_bytes))
    for m in metas:
        print(f"{m.path} size={m.size} entropy={m.entropy:.3f} header={m.header_hex}")

    # greedy assembly
    assembled = greedy_assemble(files_bytes, min_overlap=args.min_overlap, max_overlap=args.max_overlap)

    outdir = Path(args.outdir)
    outdir.mkdir(parents=True, exist_ok=True)
    manifest = []
    for idx,frag in enumerate(assembled, start=1):
        outpath = outdir / f"stitched_{idx:03d}.bin"
        with outpath.open("wb") as f:
            f.write(frag['bytes'])
        manifest.append({'out_file': str(outpath), 'size': len(frag['bytes']), 'sources': ";".join(frag['sources'])})
        print("Wrote", outpath, "size", len(frag['bytes']), "sources", frag['sources'])

    # write manifest CSV
    import csv
    manifest_path = outdir / "stitch_manifest.csv"
    with manifest_path.open("w", newline='', encoding='utf-8') as mf:
        writer = csv.DictWriter(mf, fieldnames=['out_file','size','sources'])
        writer.writeheader()
        for r in manifest:
            writer.writerow(r)
    print("Finished. Manifest:", manifest_path)

if __name__ == "__main__":
    main()

```
